{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc2df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR,ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from scipy import signal\n",
    "import os\n",
    "from collections import Counter\n",
    "import random\n",
    "# os.environ[\"MP_DUPLICATE_LIB_OK\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9dd155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1682"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "29*58*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e3103",
   "metadata": {},
   "source": [
    "## Call model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e539c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_lstm(nn.Module):\n",
    "    def __init__(self,chunk =10,sigmoid_state=True,len_input = 16,outputa = 50):\n",
    "        super().__init__()\n",
    "        if chunk == 10:\n",
    "            sep = 384//4 # due to 2 maxPool1d Kernel_size = 2\n",
    "        if chunk == 50:\n",
    "            sep = 1280\n",
    "        if chunk == 100:\n",
    "            sep = 3200//4\n",
    "        if chunk == 200:\n",
    "            sep = 6400//4\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(chunk,128)\n",
    "        \n",
    "        self.biLSTM = nn.LSTM(chunk,128,bidirectional=True)\n",
    "        self.linear1 = nn.Linear(2*3712,128)\n",
    "        self.linear2 = nn.Linear(128,outputa)\n",
    "        self.sigmoid  = nn.Sigmoid()\n",
    "        self.softmax  = nn.Softmax(dim=1)\n",
    "        self.sigmoid_state = sigmoid_state\n",
    "    def forward(self,x):\n",
    "        \n",
    "        out1,(hn,cn) = self.biLSTM(x)\n",
    "        out1 = out1.flatten()\n",
    "        output = self.linear1(out1)\n",
    "        y_final = self.linear2(output)\n",
    "        \n",
    "        if self.sigmoid_state:\n",
    "            y_final = self.sigmoid(y_final)\n",
    "        else:\n",
    "            y_final = self.softmax(y_final)\n",
    "\n",
    "        return y_final\n",
    "    \n",
    "    \n",
    "class cnn_mlp(nn.Module): #slow af\n",
    "    def __init__(self,chunk =10,sigmoid_state=True,len_input = 16,outputa = 50):\n",
    "        super().__init__()\n",
    "        if chunk == 10:\n",
    "            sep = 384//4 # due to 2 maxPool1d Kernel_size = 2\n",
    "        if chunk == 50:\n",
    "            sep = 1280\n",
    "        if chunk == 100:\n",
    "            sep = 3200//4\n",
    "        if chunk == 200:\n",
    "            sep = 6400//4\n",
    "        self.model = nn.Sequential(nn.Conv2d(1,64,4),\n",
    "                                   nn.Conv2d(64,128,4),\n",
    "                                #    nn.Conv2d(512,1024,4),\n",
    "                                   nn.Flatten(),\n",
    "                                   nn.LazyLinear(512),\n",
    "                                   nn.LazyLinear(256),\n",
    "                                   nn.LazyLinear(64),\n",
    "                                   nn.LazyLinear(outputa),\n",
    "                                   nn.Sigmoid()\n",
    "                                   \n",
    "                                    )\n",
    "    def forward(self,x):\n",
    "        output= self.model(x)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 csv_path,\n",
    "                 batch=16,\n",
    "                 chunk = 100,\n",
    "                 vocab_path = \"../Sign_Language_Detection/label.json\",\n",
    "                table:bool = False,\n",
    "                dataframe=None):\n",
    "        \n",
    "        \n",
    "        with open(vocab_path,\"r\") as f:\n",
    "            compare = json.load(f)\n",
    "        self.vocab = len(compare)\n",
    "        \n",
    "        if not table:\n",
    "            self._data_csv = pd.read_csv(csv_path)\n",
    "        else:\n",
    "            self._data_csv = dataframe\n",
    "        \n",
    "        \n",
    "        self._data_csv = self._data_csv[~(self._data_csv.Label.isin([ \"cooldown\",\"error_redo\",\"break_time\",]))]\n",
    "        self._data_csv[\"Label\"] = self._data_csv[\"Label\"].apply(lambda x:compare[x])\n",
    "        self.train_data = self.convert_data_csv_train(self._data_csv,compare,segment=chunk,range_data=25)\n",
    "        # print(len(self.train_data))\n",
    "        \n",
    "        print(self.train_data.size())\n",
    "        fity = []\n",
    "        normal =[]\n",
    "        for i in self.train_data:\n",
    "            # print(i[:,-1][0])\n",
    "            if int(i[:,-1][-1]) == 0:\n",
    "                fity.append(i)\n",
    "            else:\n",
    "                normal.append(i)\n",
    "        # print(len(fity))\n",
    "        # print(len(normal))\n",
    "        fity = random.sample(fity,int(len(fity) * 0.5))\n",
    "        normal.extend(fity)\n",
    "        self.train_data = normal\n",
    "        self.nums  = len(self.train_data)\n",
    "        self.answer_transform = []\n",
    "    \n",
    "        self.train_data = torch.tensor([i.tolist() for i in self.train_data])\n",
    "        \n",
    "        \n",
    "        for i in range(0,len(self.train_data)):\n",
    "            # print(self.train_data)\n",
    "            \n",
    "            \n",
    "            dummy = torch.zeros(self.vocab)\n",
    "            ct = Counter(self.train_data[i][:,-1].tolist()).most_common()\n",
    "            if len(ct) == 2 and ct[0][0] ==0:\n",
    "                idx,count = Counter(self.train_data[i][:,-1].tolist()).most_common()[1]\n",
    "            else:\n",
    "                idx,count = Counter(self.train_data[i][:,-1].tolist()).most_common()[0]\n",
    "            \n",
    "            dummy[int(idx)] = 1\n",
    "            self.answer_transform.append(dummy)\n",
    "        \n",
    "        self.train_data = self.train_data[:,:,:-1]\n",
    "        self.nums,self.segment,self.input = self.train_data.size()\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        inputs = self.train_data[index]\n",
    "        answer = self.answer_transform[index]\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            return inputs.to(torch.float32).movedim(1,0).to(\"cuda\"),answer.to(\"cuda\")\n",
    "        else:\n",
    "            return inputs.to(torch.float32).movedim(1,0),answer\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.nums\n",
    "    \n",
    "    def len_answer(self):\n",
    "        return self.vocab\n",
    "    \n",
    "    def data_info(self):\n",
    "        return self.nums,self.segment,self.input,self.train_data\n",
    "    \n",
    "    \n",
    "    def complementary_filter(self,gyro,gyro2,gyro3, angle_meas, dt, alpha=0.02):\n",
    "\n",
    "            angle_est = np.zeros_like(gyro)\n",
    "            angle_est[0] = angle_meas[0]  # initialize\n",
    "            for i in range(1, len(gyro)):\n",
    "                gyro_all = math.sqrt(gyro[i]**2 +gyro2[i]**2 + gyro3[i]**2) * 180/math.pi\n",
    "                gyro_pred = angle_est[i-1] +  gyro_all * dt\n",
    "                angle_est[i] = alpha * gyro_pred + (1 - alpha) * angle_meas[i]\n",
    "            return angle_est\n",
    "\n",
    "    def data_processing(self,train,x,x2,x3,y,alpha):\n",
    "            gyro = train[x].to_list() \n",
    "            gyro2 = train[x2].to_list()\n",
    "            gyro3 = train[x3].to_list()\n",
    "            angle_meas = train[y].to_list()\n",
    "\n",
    "            angle_est = self.complementary_filter(gyro,gyro2,gyro3,angle_meas,alpha)\n",
    "            return angle_est\n",
    "        \n",
    "        \n",
    "    def kalman_cal(self,data,col):\n",
    "\n",
    "        sensor_data = data[col].values.tolist()\n",
    "\n",
    "        kf = KalmanFilter()\n",
    "\n",
    "        filtered_data = []\n",
    "        for measurement in sensor_data:\n",
    "            estimate = kf.update(measurement)\n",
    "            filtered_data.append(estimate)\n",
    "\n",
    "        return filtered_data\n",
    "    \n",
    "    def convert_data_csv_train(self,data,compare,segment=50,range_data = 0):\n",
    "\n",
    "        datta = []\n",
    "        previous = None\n",
    "        samples = []\n",
    "        abc= []\n",
    "        print(\"extrac Value\")\n",
    "        # for i in tqdm(data.values,total = len(data)):\n",
    "            \n",
    "        #     if i[-1] != previous:\n",
    "        #         if previous == None:\n",
    "        #             samples.append(i)\n",
    "        #             previous = i[-1]\n",
    "        #         else:\n",
    "        #             datta.append(samples)\n",
    "        #             samples = []\n",
    "        #             previous = i[-1]\n",
    "        #     elif i[-1] == previous:\n",
    "        #         samples.append(i)\n",
    "        \n",
    "        data['group_id'] = (data['Label'] != data['Label'].shift()).cumsum()\n",
    "        grouped_dfs = [g.drop(columns='group_id').values for _, g in data.groupby('group_id')]\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"len(data): \",len(grouped_dfs))\n",
    "        print(\"filter Value\")\n",
    "        all_data = []\n",
    "        for i in tqdm(grouped_dfs,total = len(grouped_dfs)):\n",
    "            if len(i) > range_data:\n",
    "                all_data.append(i)\n",
    "\n",
    "\n",
    "        print(\"pad&mean Value\")\n",
    "\n",
    "        real = []\n",
    "        for i in tqdm(all_data,total = len(all_data)):\n",
    "            segment = segment\n",
    "            if len(i) < segment:\n",
    "                tensor_df = (torch.tensor(i))\n",
    "                n,b = tensor_df.size()\n",
    "                padded_tensor = torch.nn.functional.pad(tensor_df, pad=(0, 0, segment-n, 0), mode='constant', value=0)\n",
    "                # print(padded_tensor.size())\n",
    "                real.append(padded_tensor.tolist())\n",
    "            else:\n",
    "                step = int(np.ceil(len(i)//segment))\n",
    "                temp = []\n",
    "                for k in range(segment):\n",
    "                    temp.append(torch.mean(torch.tensor(i[k*step:(k+1)*step]),dim=0).tolist())\n",
    "                real.append(temp)\n",
    "\n",
    "        train_data = torch.tensor(real)\n",
    "        return train_data\n",
    "    \n",
    "        \n",
    "class KalmanFilter:\n",
    "    def __init__(self, process_variance=1e-4, measurement_variance=0.01, initial_estimate=0.0, initial_error=1.0):\n",
    "        self.Q = process_variance     # ความไม่แน่นอนของกระบวนการ (process noise)\n",
    "        self.R = measurement_variance # ความไม่แน่นอนของการวัด (sensor noise)\n",
    "        self.x = initial_estimate     # ค่าประมาณเริ่มต้น (initial estimate)\n",
    "        self.P = initial_error        # ความไม่แน่นอนของค่าประมาณเริ่มต้น (initial error)\n",
    "    \n",
    "    def update(self, measurement):\n",
    "        # Predict step\n",
    "        self.P += self.Q\n",
    "\n",
    "        # Kalman Gain\n",
    "        K = self.P / (self.P + self.R)\n",
    "\n",
    "        # Update estimate with measurement\n",
    "        self.x += K * (measurement - self.x)\n",
    "\n",
    "        # Update error\n",
    "        self.P *= (1 - K)\n",
    "\n",
    "        return self.x\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "# cnn = cnn_lstm()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec47bb9",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd6fb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"collect_data/20250618_141745_test_sus_1_sensor.csv\")\n",
    "\n",
    "data_normal = data[~(data[\"Label\"] == \"nothing\")]\n",
    "notthin = data[(data[\"Label\"] == \"nothing\")].sample(n=5000)\n",
    "data = pd.concat([data_normal,notthin]).reset_index(drop=True)\n",
    "data = data[~data.Label.isin([\"error_redo\",\"break_time\",\"cooldown\"])].reset_index(drop=True)\n",
    "# data = data.sample(n=len(data),random_state=42).reset_index(drop=True)\n",
    "# column = [ \n",
    "#     \"motion_detected\",\n",
    "#     \"roll_deg\", \n",
    "#     \"pitch_deg\", \n",
    "#     \"yaw_deg\",\n",
    "#     \"filtered_accel_x_(m/s²)\", \n",
    "#     \"filtered_accel_y_(m/s²)\", \n",
    "#     \"filtered_accel_z_(m/s²)\",\n",
    "#     \"filtered_gyro_x_(deg/s)\",\n",
    "#     \"filtered_gyro_y_(deg/s)\",\n",
    "#     \"filtered_gyro_z_(deg/s)\",\n",
    "#     \"accel_magnitude\", \n",
    "#     \"gyro_magnitude\",\n",
    "#     \"adc0\", \n",
    "#     \"adc1\", \n",
    "#     \"adc2\", \n",
    "#     \"adc3\", \n",
    "#     \"adc4\"\n",
    "#     \"Label\"]\n",
    "train = data.drop(columns = [\"Label\",\"timestamp_ms\"])\n",
    "test = data[[\"Label\"]][\"Label\"]\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c7af9c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Sign_Language_Detection/label.json\",\"r\") as f:\n",
    "    compare = json.load(f)\n",
    "data = pd.read_csv(r\"F:\\Hybridmodel-project\\Sign_Language_Detection\\collect_data\\20250624_101902_ชาตชาย24062025_sensor.csv\")\n",
    "data[\"Label\"] = data[\"Label\"].apply(lambda x:compare[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "208e3876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paa(df, segments=50):\n",
    "    n = len(df)\n",
    "    frame_size = n // segments\n",
    "    reduced = []\n",
    "\n",
    "    for i in range(segments):\n",
    "        start = i * frame_size\n",
    "        end = (i + 1) * frame_size if i < segments - 1 else n\n",
    "        reduced.append(df.iloc[start:end].mean())\n",
    "\n",
    "    return pd.DataFrame(reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377b4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.signal import medfilt\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "# name = ['gyro_y', 'gyro_z', 'accel_x', 'accel_y', 'accel_z', 'angle_x',\n",
    "#        'angle_y', 'angle_z', 'adc0', 'adc1', 'adc2', 'adc3', 'adc4',]\n",
    "# fig ,ax = plt.subplots(5,4,figsize=(25, 25))\n",
    "\n",
    "# data[\"Label\"] = data[\"Label\"].apply(lambda x:compare[x])\n",
    "data = new_df\n",
    "\n",
    "# for i in range(5):\n",
    "#     # data[f\"adc{i}_fil\"] = data_processing(data,\"filtered_gyro_x_(deg/s)\",\"filtered_gyro_y_(deg/s)\",\"filtered_gyro_z_(deg/s)\",f\"adc{i}\",0.02)\n",
    "#     data[f\"adc{i}_fil\"] = kalman_cal(data,f\"adc{i}\")\n",
    "    \n",
    "data[f\"gyrox\"] = kalman_cal(data,\"gyro_x\")  \n",
    "data[f\"gyroy\"] = kalman_cal(data,\"gyro_y\")  \n",
    "data[f\"gyroz\"] = kalman_cal(data,\"gyro_z\")  \n",
    "    \n",
    "\n",
    "# inputs = data[name].values\n",
    "data\n",
    "n = 0\n",
    "swgment = 5000\n",
    "checker = []\n",
    "for k in range(1,3):\n",
    "    # trains = data[data[\"Label\"] == k].reset_index(drop=True)\n",
    "    trains = data\n",
    "    # trains2 = data[data[\"Label\"] == k+1].reset_index(drop=True)\n",
    "    inputs = trains.values\n",
    "    # imputs2 = trains2.values\n",
    "    for i in range(len(trains.columns)):\n",
    "        # if \"gyro\" in data.columns[i]:\n",
    "\n",
    "        x = torch.arange(len(torch.tensor(inputs[:swgment])))\n",
    "        plt.figure(figsize=(25,10))\n",
    "        plt.plot(x,(torch.tensor(inputs[:swgment])[:,i]))\n",
    "        # plt.plot(x,medfilt(torch.tensor(inputs[:swgment])[:,i],kernel_size=5))\n",
    "        \n",
    "        \n",
    "        # plt.plot(x,(torch.tensor(imputs2[:swgment])[:,i]))\n",
    "        # # plt.plot(x,medfilt(torch.tensor(imputs2[:swgment])[:,i],kernel_size=9))\n",
    "        plt.title(f\"code {k} compare to {k+1} using {data.columns[i]}\")\n",
    "        # checker.append(torch.tensor(inputs[:swgment])[:,i])\n",
    "        # ax[n,i%4].plot(x,torch.tensor(inputs[:])[:,i])\n",
    "        # ax[n,i%4].set_title(name[i])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c49c8ab",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CustomDataset(\"collect_data/20250624_101902_ชาตชาย24062025_sensor.csv\",vocab_path=\"../Sign_Language_Detection/label.json\",chunk=50,table=True,dataframe=base_df)\n",
    "# data_answer = []\n",
    "# for inputs,answer in tqdm(train_dataset):\n",
    "      \n",
    "#       # try:\n",
    "#           # print(torch.tensor(inputs[i:i+chunk]).size())\n",
    "#       data_answer.append(answer)\n",
    "\n",
    "\n",
    "# # train.size()\n",
    "\n",
    "# with open(\"rollback.json\",'r') as f:\n",
    "#     ct = json.load(f)\n",
    "    \n",
    "    \n",
    "# print(len(data_answer))\n",
    "# label_list = [int(torch.argmax(i)) for i in data_answer] \n",
    "# nv = Counter(label_list)\n",
    "# print(nv.most_common())\n",
    "\n",
    "\n",
    "\n",
    "# not_eng = []\n",
    "# for i,v in nv.most_common():\n",
    "#     print(f\"{ct[str(i)]} : {v} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7565c356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413849\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "base_df = pd.DataFrame()\n",
    "for i in glob.glob(r\"./collect_data/new_data/*\"):\n",
    "    df = pd.read_csv(rf\"{i}\")\n",
    "    base_df = pd.concat([base_df,df])\n",
    "print(len(base_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223ea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wayupuk sommuang\\AppData\\Local\\Temp\\ipykernel_7128\\2625305297.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._data_csv[\"Label\"] = self._data_csv[\"Label\"].apply(lambda x:compare[x])\n",
      "C:\\Users\\wayupuk sommuang\\AppData\\Local\\Temp\\ipykernel_7128\\2625305297.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['group_id'] = (data['Label'] != data['Label'].shift()).cumsum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extrac Value\n",
      "len(data):  5911\n",
      "filter Value\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54477362c11f4662894c34bb58ac1728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5911 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad&mean Value\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb0239df6434b33bc529d74062f5b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3097 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3097, 50, 30])\n",
      " data train = 3024 with 29 feature\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5616091053d45e993187e142ffbb411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 0 loss = 0.021960875019431114 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588c0de79c06479a963913c613ab9e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 1 loss = 0.013334318995475769 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d2f70c4ba248609c173042db7b7f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 2 loss = 0.0103964414447546 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6fb0787a98a4d3988695a5c00d4ea53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 3 loss = 0.008496247231960297 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7aaf2b771204b4f93bd1129c285764c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 4 loss = 0.007009138353168964 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec131a8f4824ad4b1b5f71c52b73b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 5 loss = 0.00577222416177392 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad2cb660ddb47958fbd192cd7caf66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 6 loss = 0.004534328822046518 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1514f67c0714d38abff9c56bcdb6bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 7 loss = 0.0035071419551968575 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b42297be8ed412ba779f31580b1b87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 8 loss = 0.002704707207158208 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d61f9039f144cf8491c64f6e6cb095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 9 loss = 0.002168481471017003 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1efca2b2f8b4919aaf92c0c17c4a06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 10 loss = 0.0018171980045735836 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d4e19694c343caa9e6eaba75eff3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 11 loss = 0.0015705639962106943 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdde00909b64803a14406c708918ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 12 loss = 0.0013469854602590203 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c722590726e448dabe492bcecea44cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 13 loss = 0.001177908736281097 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5dad8a751944918e83e566eaf68c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 14 loss = 0.0010155242634937167 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5f1bf493c04febacc4380484408207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 15 loss = 0.0008947085007093847 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141024ad3c714ef0b35f5f50d0370067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 16 loss = 0.0008057778468355536 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7ff692fff1451db0a4c7144934d343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 17 loss = 0.0006622508517466486 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71c41145a9646ab8cccd22a75f26dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 18 loss = 0.0005935829831287265 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681f977eef03418db21f6915d3df8d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 19 loss = 0.000568925985135138 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50bfd205c9e48a1907af18963912522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 20 loss = 0.00040951266419142485 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2766c3d4688f416cacf5762144b058f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 21 loss = 0.00033010070910677314 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d05b51e6cdb44489d5cb4b36a16e4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 22 loss = 0.0002810829319059849 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cbc150bb1e4a31870df196e60700ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 23 loss = 0.00024362401745747775 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4fd8836d314ab382de424eeacaba24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 24 loss = 0.00022015209833625704 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679eceae820942c1bd28aa018c8c81cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 25 loss = 0.00020440781372599304 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd39c00620145f39dc588ee1ca2b863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 26 loss = 0.00015271302254404873 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c11b95170b4480ab611d12570a19995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 27 loss = 0.00011158640700159594 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2d1cfb5029453cb69e3a733a6de08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 28 loss = 7.735626422800124e-05 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082213ee200d46f3a767fae04c95e448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 29 loss = 5.356986366678029e-05 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c7c546ea604c3285bcebdbacbbfeb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 30 loss = 3.903847755282186e-05 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ee0538f3f149d18a9aa68dc8dd9d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 31 loss = 2.8546255634864792e-05 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8890806b4eeb4571a0ac9717cd985456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 32 loss = 2.1915680918027647e-05 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fd359512574e1e976670eae99613a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 33 loss = 1.7412612578482367e-05 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f208f2c74b9437eb104bb519ab6bf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 34 loss = 1.3100206160743255e-05 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e26720119d94c6893b7c5a47f57cc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 35 loss = 9.47682110563619e-06 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ec98f4260b452dab0bc9b33b0a9e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 36 loss = 1.0243714314128738e-05 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea09f6e4fcf4fbab40158bd76791e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 37 loss = 8.607922609371599e-06 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8f2015b5a247fea8ff59f22ad47435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 38 loss = 7.241454113682266e-06 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea571fb8d079495995a91e316bc9f4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 39 loss = 6.616789960389724e-06 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11a5bc41ae94317a9c0c7c00e1e9866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 40 loss = 6.3824445533100516e-06 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc43c4bd22c84b07b83ba4dc7aa1bc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 41 loss = 6.155259598017437e-06 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e0600c8e1447428a422d5c4cdc39fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 42 loss = 6.0477432271e-06 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41dd6b011b9a4b69ad73a98abded4aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 43 loss = 5.968095138086937e-06 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389fbf64a92c45a397c4269378445c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 44 loss = 5.888485247851349e-06 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4df67815bd433abd0860b45fd62b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 45 loss = 5.8547361732053105e-06 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0304e99d4045deae5b3e21da4a5265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 46 loss = 6.8361791818460915e-06 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0708643b0a4fe88156dd8953942a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 47 loss = 5.9812759900523815e-06 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c217e3bff7fe4b22ada9b7fe28920288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 48 loss = 5.8186524256598204e-06 with lr = [2e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04959e3a024410f8c72729c9b05596e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 49 loss = 5.773445991508197e-06 with lr = [2e-05]\n",
      "best loss at epoch = 50 with 5.773445991508197e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "epoch = 50\n",
    "lr = 2e-5\n",
    "chunk = 50\n",
    "e = 0\n",
    "best_loss = 0\n",
    "path_save = \"../Sign_Language_Detection/model/Version1\"\n",
    "num_still = 0\n",
    "sigoid_state = True\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = CustomDataset(\"collect_data/20250624_101902_ชาตชาย24062025_sensor.csv\",vocab_path=\"../Sign_Language_Detection/label.json\",chunk=50,table=True,dataframe=base_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "len_output = train_dataset.len_answer()\n",
    "len_input = train_dataset.data_info()[-2]\n",
    "train_dataset = DataLoader(train_dataset,batch_size=batch_size)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  cnn = cnn_lstm(chunk,sigmoid_state=sigoid_state,len_input=len_input,outputa=len_output).to(\"cuda\")\n",
    "else:\n",
    "  cnn = cnn_lstm(chunk,sigmoid_state=sigoid_state,len_input=len_input,outputa=len_output)\n",
    "\n",
    "\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#   cnn = cnn_mlp(chunk,sigmoid_state=sigoid_state,len_input=len_input,outputa=len_output).to(\"cuda\")\n",
    "#   print(\"use cuda\")\n",
    "# else:\n",
    "#   cnn = cnn_mlp(chunk,sigmoid_state=sigoid_state,len_input=len_input,outputa=len_output)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(cnn.parameters(),lr=lr,weight_decay=0.0001)\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min',patience =3  ,min_lr = 5e-6,factor=0.5)\n",
    "print(f\" data train = {int(len(train_dataset)*batch_size)} with {len_input} feature\")\n",
    "\n",
    "\n",
    "n = 0\n",
    "for param in cnn.parameters():\n",
    "  param.requires_grad=True\n",
    "cnn.train()\n",
    "\n",
    "for k in range(1,epoch+1):\n",
    "    loss_total = 0\n",
    "    data_answer = []\n",
    "    for inputs,answer in tqdm(train_dataset):\n",
    "      answer = answer[0]\n",
    "      data_answer.append(answer)\n",
    "      # print(inputs.size())\n",
    "      output = cnn(inputs)\n",
    "      optimizer.zero_grad()\n",
    "      if torch.argmax(answer).item() == 0:\n",
    "        loss = criterion(output,answer)\n",
    "      else:\n",
    "        loss = criterion(output,answer)\n",
    "      \n",
    "      \n",
    "      # print(f\"answer = {output[0]}\")\n",
    "      # print(f\"output = {answer[0]}\")\n",
    "      # print(loss)\n",
    "      # break\n",
    "      \n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      # if k == 15:\n",
    "      #   print(f\"Label = {torch.argmax(answer)} model answer = {torch.argmax(output[0])}\")\n",
    "\n",
    "      loss_total += loss\n",
    "      n+=1\n",
    "      # if n == 5:\n",
    "      #   break\n",
    "      # except:\n",
    "      #   print(\"face error\")\n",
    "        \n",
    "    if best_loss == 0 or best_loss > loss_total/len(train_dataset):\n",
    "      best_loss = loss_total/len(train_dataset)\n",
    "      state_dict = cnn.state_dict()\n",
    "      e = k\n",
    "      num_still = 0\n",
    "    else:\n",
    "      num_still +=1\n",
    "      \n",
    "    scheduler.step(loss_total/len(train_dataset))\n",
    "    num_still = 0\n",
    "    if num_still >= 3:\n",
    "      print(\"step up to learning = \",scheduler.get_last_lr())\n",
    "      break\n",
    "      \n",
    "    \n",
    "    print(f\"epoch number {k} loss = {loss_total/len(train_dataset)} with lr = {scheduler.get_last_lr()}\")\n",
    "    # break\n",
    "print(f\"best loss at epoch = {e} with {best_loss}\")\n",
    "torch.save(state_dict,f\"{path_save}/model_epoch_{e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23403c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(r\"./label.json\",'r') as f:\n",
    "    ct = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb06712",
   "metadata": {},
   "outputs": [],
   "source": [
    "datac = {}\n",
    "for i,v in ct.items():\n",
    "    datac[v] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0510c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rollback.json\",'w') as f:\n",
    "    json.dump(datac,f,indent=1,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a0285556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1636\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_df = pd.DataFrame()\n",
    "for i in glob.glob(r\"./collect_data/new_data/*test_*.csv\"):\n",
    "    df = pd.read_csv(rf\"{i}\")\n",
    "    test_df = pd.concat([test_df,df])\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53a8a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(rf\"F:\\Hybridmodel-project\\Sign_Language_Detection\\collect_data\\20250715_111750_DATA_INDICATOR_sensor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = base_df.iloc[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9559fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extrac Value\n",
      "len(data):  201\n",
      "filter Value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wayupuk sommuang\\AppData\\Local\\Temp\\ipykernel_7128\\2625305297.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._data_csv[\"Label\"] = self._data_csv[\"Label\"].apply(lambda x:compare[x])\n",
      "C:\\Users\\wayupuk sommuang\\AppData\\Local\\Temp\\ipykernel_7128\\2625305297.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['group_id'] = (data['Label'] != data['Label'].shift()).cumsum()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3eb5057fb44d919ea4f43e46b22eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad&mean Value\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e48d3d633b4aa883877ff9984c8970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 50, 30])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430681546ad64dc99fbe3b9eab352505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# test_df = pd.DataFrame()\n",
    "# for i in glob.glob(r\"./collect_data/new_data/*test_*.csv\"):\n",
    "#     df = pd.read_csv(rf\"{i}\")\n",
    "#     test_df = pd.concat([test_df,df])\n",
    "# print(len(test_df))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = CustomDataset(\"collect_data/20250624_131408_พชชาภา24062025_sensor.csv\",vocab_path=\"../Sign_Language_Detection/label.json\",chunk=chunk,table=True,dataframe=test_df)\n",
    "test_dataset = DataLoader(test_dataset,batch_size=1)\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "# notthing_c = 0\n",
    "# action = []\n",
    "# start = True\n",
    "with torch.no_grad():\n",
    "    # for i in tqdm(range(0,int(len(inputs)),chunk)):\n",
    "    #     try:\n",
    "    #         # print(torch.tensor(inputs[i:i+chunk]).size())\n",
    "    #         output = cnn(torch.tensor(inputs[i:i+chunk]).movedim(1,0).unsqueeze(0).to(\"cuda\"))\n",
    "    #         y_pred.append(torch.argmax(output).item())\n",
    "    #         y_true.append(torch.argmax(answer[int(i/chunk)]).item())\n",
    "            \n",
    "    #         with torch.no_grad():\n",
    "    for inputs,answer in tqdm(test_dataset):\n",
    "        # X_scaled = scaler.fit_transform(inputs.cpu()[0])\n",
    "        # X_pca = pca.fit_transform(X_scaled)\n",
    "      \n",
    "        # inputs = torch.Tensor(X_pca).unsqueeze(0).to(\"cuda\")\n",
    "        output = cnn(inputs)\n",
    "        # loss = criterion(output[0],answer[0])\n",
    "        # if torch.argmax(answer,dim=1)[0].item() != 0:\n",
    "            # print((torch.argmax(answer,dim=1)).tolist())\n",
    "        y_pred += [(torch.argmax(output,dim=0)).tolist()]\n",
    "        y_true += (torch.argmax(answer,dim=1)).tolist()\n",
    "        \n",
    "        # if torch.argmax(output).item() != 11:\n",
    "        #     action.append(torch.argmax(output).item())\n",
    "        #     start = False\n",
    "        # else:\n",
    "        #     notthing_c+=1\n",
    "        \n",
    "        # if notthing_c >= 10 and not start :\n",
    "        #     print(action[-1])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e479d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.argmax(answer,dim=1)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9202f260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score     0.7339449541284404\n",
      "recal score  0.7339449541284404\n",
      "acc score    0.7339449541284404\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,recall_score,accuracy_score\n",
    "f1_scores = f1_score(y_true, y_pred, average=\"micro\")\n",
    "print(\"f1 score    \",f1_scores)\n",
    "recall_scores = recall_score(y_true, y_pred, average=\"micro\")\n",
    "print(\"recal score \",recall_scores)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(\"acc score   \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in zip(y_true,y_pred):\n",
    "    if i==v:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca7a658",
   "metadata": {},
   "source": [
    "# model2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd93cde",
   "metadata": {},
   "source": [
    "## text embedding way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f83f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self,outputa=50):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(10000,256)\n",
    "        self.l2 = nn.Linear(256,outputa)\n",
    "    def forward(self,x):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class cnn_lstm_multi(nn.Module):\n",
    "    def __init__(self,chunk =10,sigmoid_state=True,len_input = 16,outputa = 50):\n",
    "        super().__init__()\n",
    "        if chunk == 10:\n",
    "            sep = 384//4 # due to 2 maxPool1d Kernel_size = 2\n",
    "        if chunk == 50:\n",
    "            sep = 1280\n",
    "        if chunk == 100:\n",
    "            sep = 3200//4\n",
    "        if chunk == 200:\n",
    "            sep = 6400//4\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(chunk,128)\n",
    "        \n",
    "        self.biLSTM = nn.LSTM(chunk,128,bidirectional=True)\n",
    "        self.linear1 = nn.Linear(2*3712,128)\n",
    "        self.linear2 = nn.Linear(128,10000)\n",
    "        self.sigmoid  = nn.Sigmoid()\n",
    "        self.softmax  = nn.Softmax(dim=1)\n",
    "        self.sigmoid_state = sigmoid_state\n",
    "        self.decoder = decoder(outputa)\n",
    "    def forward(self,x):\n",
    "        \n",
    "        out1,(hn,cn) = self.biLSTM(x)\n",
    "        out1 = out1.flatten()\n",
    "        output = self.linear1(out1)\n",
    "        y_final = self.linear2(output)\n",
    "        \n",
    "        if self.sigmoid_state:\n",
    "            y_final = self.sigmoid(y_final)\n",
    "        else:\n",
    "            y_final = self.softmax(y_final)\n",
    "\n",
    "        return y_final\n",
    "    \n",
    "    def nums_to_word(self,x):\n",
    "        word = self.decoder(x)\n",
    "        \n",
    "        return word\n",
    "    \n",
    "    def words_to_nums(self,x):\n",
    "        word = self.emb(torch.tensor(compare[x],dtype=torch.long))\n",
    "        \n",
    "        return word\n",
    "    \n",
    "    \n",
    "cnn = cnn_lstm_multi()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "15a059bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Sign_Language_Detection/turnback.json\",\"r\") as f:\n",
    "    turnback = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d117b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch = 20\n",
    "lr = 0.002\n",
    "chunk = 50\n",
    "inputs = train.values.tolist()\n",
    "answer = test\n",
    "\n",
    "cnn = cnn_lstm_multi(chunk)\n",
    "optimizer = torch.optim.AdamW(cnn.parameters())\n",
    "cnn.emb.requires_grad_ = False\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for param in cnn.parameters():\n",
    "  param.requires_grad=True\n",
    "cnn.train()\n",
    "for i in range(epoch):\n",
    "    # output = cnn(torch.tensor(inputs[i],dtype=torch.float64).unsqueeze(0))\n",
    "    for i in range(0,int(len(inputs)),chunk):\n",
    "        # print(torch.tensor(inputs[i:i+chunk]).size())\n",
    "        try:\n",
    "          output = cnn(torch.tensor(inputs[i:i+chunk]).movedim(1,0).unsqueeze(0))\n",
    "          optimizer.zero_grad()\n",
    "          # print(output)\n",
    "          loss = criterion(output[0],cnn.words_to_nums(answer[i//10]))\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        except:\n",
    "          print(\"\")\n",
    "    print(f\"loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ca12af54",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['gyro_x', 'gyro_y', 'gyro_z', 'accel_x', 'accel_y', 'accel_z', 'angle_x', 'angle_y', 'angle_z'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollect_data/test_v2_sensor.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvocab_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../Sign_Language_Detection/onetoten.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[81], line 76\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[1;34m(self, data, batch, chunk, vocab_path)\u001b[0m\n\u001b[0;32m     74\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([data_normal,notthin])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     75\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m~\u001b[39mdataset\u001b[38;5;241m.\u001b[39mLabel\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror_redo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbreak_time\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcooldown\u001b[39m\u001b[38;5;124m\"\u001b[39m])]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 76\u001b[0m dataset  \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m    \n\u001b[0;32m     78\u001b[0m dataset[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgyrox\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkalman_cal(dataset,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgyro_x\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m     79\u001b[0m dataset[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgyroy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkalman_cal(dataset,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgyro_y\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\f5-tts\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\f5-tts\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\f5-tts\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['gyro_x', 'gyro_y', 'gyro_z', 'accel_x', 'accel_y', 'accel_z', 'angle_x', 'angle_y', 'angle_z'] not in index\""
     ]
    }
   ],
   "source": [
    "test_dataset = CustomDataset(\"collect_data/test_v2_sensor.csv\",vocab_path=\"../Sign_Language_Detection/onetoten.json\",chunk=chunk)\n",
    "test_dataset = DataLoader(test_dataset,batch_size=1)\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "# ans = []\n",
    "# for i in compare:\n",
    "#     ans.append(cnn.words_to_nums(i))\n",
    "cnn.to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "\n",
    "    for inputs,answer in tqdm(test_dataset):\n",
    "        \n",
    "        # try:\n",
    "        output = cnn(inputs)\n",
    "        \n",
    "        # ,cnn.words_to_nums(answer[i//10])\n",
    "        # y_pred += (torch.argmax(output,dim=1)).tolist()\n",
    "        a = []\n",
    "        for i in ans:\n",
    "            a.append(sum((output[0]-i.to(\"cuda\"))**2))\n",
    "        answers = torch.argmax(torch.tensor(a)).item()\n",
    "        \n",
    "        if torch.argmax(answer) != 11:\n",
    "            y_pred.append(answers)\n",
    "            \n",
    "            \n",
    "            y_true.append(torch.argmax(answer))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        # except:\n",
    "        #     print(\"hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aida",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
